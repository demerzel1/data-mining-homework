{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import cv2\n",
    "import torch\n",
    "import time\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import ToPILImage\n",
    "show=ToPILImage()\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0.1.post2\n",
      "3.7.2 (default, Dec 29 2018, 00:00:04) \n",
      "[Clang 4.0.1 (tags/RELEASE_401/final)]\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/train-labels-idx1-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "#set batch size\n",
    "batchSize=16\n",
    "\n",
    "#load data\n",
    "transform = transforms.Compose([transforms.Resize(224),transforms.ToTensor(),transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),])\n",
    "\n",
    "trainset = torchvision.datasets.FashionMNIST(root='../input/mnist/data', train=True, download=True, transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batchSize, shuffle=True, num_workers=0)\n",
    "\n",
    "testset = torchvision.datasets.FashionMNIST(root='../input/mnist/data', train=False, download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batchSize, shuffle=False, num_workers=0)\n",
    "\n",
    "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AlexNet(\n",
      "  (conv1): Conv2d(1, 96, kernel_size=(11, 11), stride=(4, 4))\n",
      "  (pool1): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(96, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool2): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv3): Conv2d(256, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv4): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (conv5): Conv2d(384, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  (pool3): MaxPool2d(kernel_size=3, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (dense1): Linear(in_features=6400, out_features=4096, bias=True)\n",
      "  (drop1): Dropout(p=0.5)\n",
      "  (dense2): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "  (drop2): Dropout(p=0.5)\n",
      "  (dense3): Linear(in_features=4096, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet,self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels=1,out_channels=96,kernel_size=11,stride=4)\n",
    "        self.pool1 = nn.MaxPool2d(kernel_size=3,stride=2)\n",
    "        self.conv2 = nn.Conv2d(in_channels=96,out_channels=256,kernel_size=5,padding=2)\n",
    "        self.pool2 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.conv3 = nn.Conv2d(in_channels=256,out_channels=384,kernel_size=3,padding=1)\n",
    "        self.conv4 = nn.Conv2d(in_channels=384, out_channels=384, kernel_size=3, padding=1)\n",
    "        self.conv5 = nn.Conv2d(in_channels=384, out_channels=256, kernel_size=3, padding=1)\n",
    "        self.pool3 = nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        self.dense1 = nn.Linear(256*5*5,4096)\n",
    "        self.drop1 = nn.Dropout(0.5)\n",
    "        self.dense2 = nn.Linear(4096,4096)\n",
    "        self.drop2 = nn.Dropout(0.5)\n",
    "        self.dense3 = nn.Linear(4096,10)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x=self.pool1(F.relu(self.conv1(x)))\n",
    "        x=self.pool2(F.relu(self.conv2(x)))\n",
    "        x=self.pool3(F.relu(self.conv5(F.relu(self.conv4(F.relu(self.conv3(x)))))))\n",
    "        x=x.view(-1,256*5*5)\n",
    "        x=self.dense3(self.drop2(F.relu(self.dense2(self.drop1(F.relu(self.dense1(x)))))))\n",
    "        return x\n",
    "\n",
    "net=AlexNet()\n",
    "print (net)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=optim.SGD(net.parameters(),lr=0.01,momentum=0.9)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training begin\n",
      "[epoch 1,imgs  1600] loss: 2.2847359  time: 141.918 s\n",
      "[epoch 1,imgs  3200] loss: 1.6495736  time: 146.801 s\n",
      "[epoch 1,imgs  4800] loss: 1.0913321  time: 144.900 s\n",
      "[epoch 1,imgs  6400] loss: 0.9178799  time: 143.618 s\n",
      "[epoch 1,imgs  8000] loss: 0.7255520  time: 141.535 s\n",
      "[epoch 1,imgs  9600] loss: 0.6970863  time: 141.132 s\n",
      "[epoch 1,imgs 11200] loss: 0.6619957  time: 141.106 s\n",
      "[epoch 1,imgs 12800] loss: 0.6470241  time: 141.109 s\n",
      "[epoch 1,imgs 14400] loss: 0.6290392  time: 142.699 s\n",
      "[epoch 1,imgs 16000] loss: 0.5692280  time: 155.037 s\n",
      "[epoch 1,imgs 17600] loss: 0.5250652  time: 166.834 s\n",
      "[epoch 1,imgs 19200] loss: 0.5419562  time: 153.761 s\n",
      "[epoch 1,imgs 20800] loss: 0.4948173  time: 167.480 s\n",
      "[epoch 1,imgs 22400] loss: 0.4833591  time: 161.161 s\n",
      "[epoch 1,imgs 24000] loss: 0.5471354  time: 144.843 s\n",
      "[epoch 1,imgs 25600] loss: 0.4870166  time: 144.650 s\n",
      "[epoch 1,imgs 27200] loss: 0.4130421  time: 144.474 s\n",
      "[epoch 1,imgs 28800] loss: 0.4299891  time: 144.697 s\n",
      "[epoch 1,imgs 30400] loss: 0.4477295  time: 144.696 s\n",
      "[epoch 1,imgs 32000] loss: 0.4404011  time: 144.991 s\n",
      "[epoch 1,imgs 33600] loss: 0.4917529  time: 144.793 s\n",
      "[epoch 1,imgs 35200] loss: 0.4453475  time: 143.750 s\n",
      "[epoch 1,imgs 36800] loss: 0.4635570  time: 143.428 s\n",
      "[epoch 1,imgs 38400] loss: 0.4259899  time: 143.857 s\n",
      "[epoch 1,imgs 40000] loss: 0.4119518  time: 143.844 s\n",
      "[epoch 1,imgs 41600] loss: 0.4230887  time: 143.937 s\n",
      "[epoch 1,imgs 43200] loss: 0.4295792  time: 143.591 s\n",
      "[epoch 1,imgs 44800] loss: 0.3917764  time: 143.202 s\n",
      "[epoch 1,imgs 46400] loss: 0.3929872  time: 144.523 s\n",
      "[epoch 1,imgs 48000] loss: 0.4039640  time: 142.997 s\n",
      "[epoch 1,imgs 49600] loss: 0.4180028  time: 142.921 s\n",
      "[epoch 1,imgs 51200] loss: 0.4124336  time: 143.268 s\n",
      "[epoch 1,imgs 52800] loss: 0.4171335  time: 142.804 s\n",
      "[epoch 1,imgs 54400] loss: 0.3884892  time: 143.020 s\n",
      "[epoch 1,imgs 56000] loss: 0.3891869  time: 143.191 s\n",
      "[epoch 1,imgs 57600] loss: 0.4136811  time: 143.556 s\n",
      "[epoch 1,imgs 59200] loss: 0.3692351  time: 143.181 s\n",
      "[epoch 2,imgs  1600] loss: 0.3401576  time: 143.554 s\n",
      "[epoch 2,imgs  3200] loss: 0.3850426  time: 143.452 s\n",
      "[epoch 2,imgs  4800] loss: 0.3690293  time: 143.453 s\n",
      "[epoch 2,imgs  6400] loss: 0.3831410  time: 143.455 s\n",
      "[epoch 2,imgs  8000] loss: 0.3415757  time: 143.519 s\n",
      "[epoch 2,imgs  9600] loss: 0.3105979  time: 143.782 s\n",
      "[epoch 2,imgs 11200] loss: 0.3484105  time: 143.888 s\n",
      "[epoch 2,imgs 12800] loss: 0.3396883  time: 143.773 s\n",
      "[epoch 2,imgs 14400] loss: 0.3653822  time: 143.883 s\n",
      "[epoch 2,imgs 16000] loss: 0.3385613  time: 143.518 s\n",
      "[epoch 2,imgs 17600] loss: 0.3303554  time: 143.803 s\n",
      "[epoch 2,imgs 19200] loss: 0.3656909  time: 143.680 s\n",
      "[epoch 2,imgs 20800] loss: 0.3367178  time: 143.835 s\n",
      "[epoch 2,imgs 22400] loss: 0.3617561  time: 144.019 s\n",
      "[epoch 2,imgs 24000] loss: 0.3505705  time: 143.582 s\n",
      "[epoch 2,imgs 25600] loss: 0.3316917  time: 143.927 s\n",
      "[epoch 2,imgs 27200] loss: 0.3345232  time: 144.220 s\n",
      "[epoch 2,imgs 28800] loss: 0.3564247  time: 145.060 s\n",
      "[epoch 2,imgs 30400] loss: 0.3409302  time: 144.543 s\n",
      "[epoch 2,imgs 32000] loss: 0.3186131  time: 145.045 s\n",
      "[epoch 2,imgs 33600] loss: 0.3368078  time: 144.441 s\n",
      "[epoch 2,imgs 35200] loss: 0.3125535  time: 144.540 s\n",
      "[epoch 2,imgs 36800] loss: 0.2775654  time: 144.389 s\n",
      "[epoch 2,imgs 38400] loss: 0.3376697  time: 144.400 s\n",
      "[epoch 2,imgs 40000] loss: 0.2936229  time: 144.286 s\n",
      "[epoch 2,imgs 41600] loss: 0.3041552  time: 144.424 s\n",
      "[epoch 2,imgs 43200] loss: 0.3500813  time: 144.289 s\n",
      "[epoch 2,imgs 44800] loss: 0.4162671  time: 144.347 s\n",
      "[epoch 2,imgs 46400] loss: 0.3363164  time: 144.039 s\n",
      "[epoch 2,imgs 48000] loss: 0.3210029  time: 143.782 s\n",
      "[epoch 2,imgs 49600] loss: 0.3402513  time: 143.599 s\n",
      "[epoch 2,imgs 51200] loss: 0.3345270  time: 143.743 s\n",
      "[epoch 2,imgs 52800] loss: 0.3198928  time: 143.467 s\n",
      "[epoch 2,imgs 54400] loss: 0.3287427  time: 143.604 s\n",
      "[epoch 2,imgs 56000] loss: 0.3264867  time: 143.677 s\n",
      "[epoch 2,imgs 57600] loss: 0.3274274  time: 144.659 s\n",
      "[epoch 2,imgs 59200] loss: 0.3157364  time: 147.429 s\n",
      "[epoch 3,imgs  1600] loss: 0.2761330  time: 147.285 s\n",
      "[epoch 3,imgs  3200] loss: 0.2431664  time: 147.924 s\n",
      "[epoch 3,imgs  4800] loss: 0.2808166  time: 146.823 s\n",
      "[epoch 3,imgs  6400] loss: 0.3428778  time: 162.460 s\n",
      "[epoch 3,imgs  8000] loss: 0.3006718  time: 158.898 s\n",
      "[epoch 3,imgs  9600] loss: 0.3121576  time: 160.655 s\n",
      "[epoch 3,imgs 11200] loss: 0.3047687  time: 156.518 s\n",
      "[epoch 3,imgs 12800] loss: 0.2873392  time: 154.730 s\n",
      "[epoch 3,imgs 14400] loss: 0.3118007  time: 172.694 s\n",
      "[epoch 3,imgs 16000] loss: 0.3026056  time: 160.673 s\n",
      "[epoch 3,imgs 17600] loss: 0.3059284  time: 157.329 s\n",
      "[epoch 3,imgs 19200] loss: 0.2953956  time: 154.699 s\n",
      "[epoch 3,imgs 20800] loss: 0.2797998  time: 179.211 s\n",
      "[epoch 3,imgs 22400] loss: 0.2856851  time: 172.712 s\n",
      "[epoch 3,imgs 24000] loss: 0.2949109  time: 180.599 s\n",
      "[epoch 3,imgs 25600] loss: 0.3140664  time: 171.214 s\n",
      "[epoch 3,imgs 27200] loss: 0.2559372  time: 177.719 s\n",
      "[epoch 3,imgs 28800] loss: 0.2796010  time: 179.628 s\n",
      "[epoch 3,imgs 30400] loss: 0.3043931  time: 166.727 s\n",
      "[epoch 3,imgs 32000] loss: 0.3092143  time: 159.381 s\n",
      "[epoch 3,imgs 33600] loss: 0.2710578  time: 163.127 s\n",
      "[epoch 3,imgs 35200] loss: 0.2968215  time: 157.772 s\n",
      "[epoch 3,imgs 36800] loss: 0.2752423  time: 151.651 s\n",
      "[epoch 3,imgs 38400] loss: 0.3038048  time: 159.768 s\n",
      "[epoch 3,imgs 40000] loss: 0.2838131  time: 168.885 s\n",
      "[epoch 3,imgs 41600] loss: 0.2817380  time: 163.238 s\n",
      "[epoch 3,imgs 43200] loss: 0.2731092  time: 162.285 s\n",
      "[epoch 3,imgs 44800] loss: 0.2875159  time: 167.232 s\n",
      "[epoch 3,imgs 46400] loss: 0.3024574  time: 160.416 s\n",
      "[epoch 3,imgs 48000] loss: 0.2688200  time: 155.244 s\n",
      "[epoch 3,imgs 49600] loss: 0.3221456  time: 156.119 s\n",
      "[epoch 3,imgs 51200] loss: 0.3139261  time: 154.720 s\n",
      "[epoch 3,imgs 52800] loss: 0.2823356  time: 153.633 s\n",
      "[epoch 3,imgs 54400] loss: 0.3108830  time: 155.175 s\n",
      "[epoch 3,imgs 56000] loss: 0.2582064  time: 153.602 s\n",
      "[epoch 3,imgs 57600] loss: 0.2769798  time: 154.411 s\n",
      "[epoch 3,imgs 59200] loss: 0.2799772  time: 154.747 s\n",
      "finish training\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#train\n",
    "print (\"training\")\n",
    "\n",
    "#we train 3 rounds\n",
    "for epoch in range(3):\n",
    "    #record time\n",
    "    start = time.time()\n",
    "    running_loss=0\n",
    "    \n",
    "    for i,data in enumerate(trainloader,0):\n",
    "        # print (inputs,labels)\n",
    "        image,label=data\n",
    "        #Variable is format of PyTorch\n",
    "        image=Variable(image)\n",
    "        label=Variable(label)\n",
    "        \n",
    "        #before every echo, clean all the grad\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # print (image.shape)\n",
    "        outputs=net(image)\n",
    "        # print (outputs)\n",
    "        loss=criterion(outputs,label)\n",
    "\n",
    "        #backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss+=loss.data\n",
    "\n",
    "        if i%100==99:\n",
    "            end=time.time()\n",
    "            print ('[epoch %d,imgs %5d] loss: %.7f  time: %0.3f s'%(epoch+1,(i+1)*16,running_loss/100,(end-start)))\n",
    "            start=time.time()\n",
    "            running_loss=0\n",
    "print (\"finish training\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the 10000 test images: 87 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#test\n",
    "net.eval()\n",
    "correct=0\n",
    "total=0\n",
    "for data in testloader:\n",
    "    images,labels=data\n",
    "    #get the output\n",
    "    outputs=net(Variable(images))\n",
    "    _,predicted=torch.max(outputs,1)\n",
    "    total+=labels.size(0)\n",
    "    correct+=(predicted==labels).sum()\n",
    "print('Accuracy of the network on the %d test images: %d %%' % (total , 100 * correct / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:test-dm]",
   "language": "python",
   "name": "conda-env-test-dm-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
